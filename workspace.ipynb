{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 数据预处理模块"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\60960\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:303: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\60960\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:305: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,TensorDataset,RandomSampler\n",
    "import os\n",
    "import urllib.request,urllib.parse\n",
    "import zipfile\n",
    "import win32api\n",
    "import random\n",
    "root_path=os.path.abspath('.')\n",
    "test_data_path='test_data.json'\n",
    "test_data=json.loads(open(test_data_path,encoding='utf-8').read())\n",
    "type_dict={'排序算法':0,'查找算法':1,'图结构':2,'树结构':3,'数字操作':4,'字符串':5,'线性表':6,'数组':7}\n",
    "userIds=[str(i) for i in sorted([int(i) for i in list(test_data.keys())])]\n",
    "updateUserIds={}\n",
    "getOldUserId=[]\n",
    "for i in range(len(userIds)):\n",
    "    updateUserIds.update({userIds[i]:i})\n",
    "    getOldUserId.append(userIds[i])\n",
    "del userIds\n",
    "case_ids=set()\n",
    "for i in test_data:\n",
    "    for j in test_data[i]['cases']:\n",
    "        case_ids.add(j['case_id'])\n",
    "case_ids=sorted(list(case_ids))\n",
    "updateCaseIds={}\n",
    "getOldCaseId=[]\n",
    "for i in range(len(case_ids)):\n",
    "    updateCaseIds.update({case_ids[i]:i})\n",
    "    getOldCaseId.append(case_ids)\n",
    "del case_ids\n",
    "new_data=[]\n",
    "for i in range(271):\n",
    "    new_data.append(test_data[getOldUserId[i]]['cases'])\n",
    "del test_data\n",
    "for i in range(271):\n",
    "    for j in range(len(new_data[i])):\n",
    "        new_data[i][j]['case_id']=updateCaseIds[new_data[i][j]['case_id']]\n",
    "userFinishCaseIds=[]\n",
    "for i in range(271):\n",
    "    tempList=[]\n",
    "    for j in range(len(new_data[i])):\n",
    "        tempList.append(new_data[i][j]['case_id'])\n",
    "    userFinishCaseIds.append(sorted(tempList))\n",
    "caseIds200=[]\n",
    "groupCaseIds=[]\n",
    "ids882=[i for i in range(882)]\n",
    "theFifthGroupCaseIds=set()\n",
    "for i in range(271):\n",
    "    ids='.'.join([str(i) for i in userFinishCaseIds[i]])\n",
    "    if len(userFinishCaseIds[i])==200 and ids not in caseIds200:\n",
    "        caseIds200.append(ids)\n",
    "        groupCaseIds.append(userFinishCaseIds[i])\n",
    "        for j in userFinishCaseIds[i]:\n",
    "            ids882[j]=-1\n",
    "for i in ids882:\n",
    "    if i!=-1:\n",
    "        theFifthGroupCaseIds.add(i)\n",
    "fifthGroupIds=list(theFifthGroupCaseIds)\n",
    "for i in range(271):\n",
    "    if 190<=len(userFinishCaseIds[i])<=200:\n",
    "        flag=False\n",
    "        for j in theFifthGroupCaseIds:\n",
    "            if j in userFinishCaseIds[i]:\n",
    "                flag=True\n",
    "                break\n",
    "        if flag:\n",
    "            for j in userFinishCaseIds[i]:\n",
    "                fifthGroupIds.append(j)\n",
    "groupCaseIds.append(sorted(list(set(fifthGroupIds))))\n",
    "del fifthGroupIds\n",
    "del theFifthGroupCaseIds\n",
    "del ids882\n",
    "del caseIds200\n",
    "groupUserIds=[[] for i in range(5)]\n",
    "ctGroupUserIds=[]\n",
    "validUserIds=[]\n",
    "for i in range(271):\n",
    "    if len(userFinishCaseIds[i])>200:\n",
    "        groupUserIds[4].append(i)\n",
    "        continue\n",
    "    if len(userFinishCaseIds[i])<=10:\n",
    "        ctGroupUserIds.append(i)\n",
    "        continue\n",
    "    if i==261:\n",
    "        groupUserIds[3].append(i)\n",
    "        continue\n",
    "    g=[]\n",
    "    for j in range(5):\n",
    "        flag=True\n",
    "        for k in userFinishCaseIds[i]:\n",
    "            if not k in groupCaseIds[j]:\n",
    "                flag=False\n",
    "                break\n",
    "        if flag:\n",
    "            g.append(j)\n",
    "    if len(g)==1:\n",
    "        groupUserIds[g[0]].append(i)\n",
    "    else:\n",
    "        ctGroupUserIds.append(i)\n",
    "for i in groupUserIds:\n",
    "    for j in i:\n",
    "        validUserIds.append(j)\n",
    "validUserIds=sorted(validUserIds)\n",
    "groupUserNum=[len(i) for i in groupUserIds]\n",
    "caseGroups=[[] for i in range(882)]\n",
    "caseUserNum=[0 for i in range(882)]\n",
    "caseUserNumInFact=[0 for i in range(882)]\n",
    "caseScoreIgnoreUndo=[0 for i in range(882)]\n",
    "caseScoreCountUndo=[0 for i in range(882)]\n",
    "caseIdsByType=[set() for i in range(8)]\n",
    "for i in range(882):\n",
    "    for j in userFinishCaseIds:\n",
    "        if i in j:\n",
    "            caseUserNumInFact[i]+=1\n",
    "    for j in range(len(groupCaseIds)):\n",
    "        if i in groupCaseIds[j]:\n",
    "            caseGroups[i].append(j)\n",
    "            caseUserNum[i]+=groupUserNum[j]\n",
    "caseFinishRate=list(np.array(caseUserNumInFact)/np.array(caseUserNum))\n",
    "for i in range(len(new_data)):\n",
    "    if i not in validUserIds:\n",
    "        continue\n",
    "    for j in range(len(new_data[i])):\n",
    "        caseScoreIgnoreUndo[new_data[i][j]['case_id']]+=new_data[i][j]['final_score']\n",
    "caseScoreCountUndo=list(np.array(caseScoreIgnoreUndo)/np.array(caseUserNum))\n",
    "caseScoreIgnoreUndo=list(np.array(caseScoreIgnoreUndo)/np.array(caseUserNumInFact))\n",
    "for i in range(len(new_data)):\n",
    "    for j in range(len(new_data[i])):\n",
    "        new_data[i][j]['case_type']=type_dict[new_data[i][j]['case_type']]\n",
    "        caseIdsByType[new_data[i][j]['case_type']].add(new_data[i][j]['case_id'])\n",
    "for i in range(len(caseIdsByType)):\n",
    "    caseIdsByType[i]=sorted(list(caseIdsByType[i]))\n",
    "def getGroupIdByUserId(userId):\n",
    "    for i in range(len(groupUserIds)):\n",
    "        if userId in groupUserIds[i]:\n",
    "            return i\n",
    "    return -1\n",
    "def getGroupIdsByCaseId(caseId):\n",
    "    gs=[]\n",
    "    for i in range(len(groupCaseIds)):\n",
    "        if caseId in groupCaseIds[i]:\n",
    "            gs.append(i)\n",
    "    return gs\n",
    "def getTypesByCaseId(caseId):\n",
    "    ts=[]\n",
    "    for i in range(len(caseIdsByType)):\n",
    "        if caseId in caseIdsByType[i]:\n",
    "            ts.append(i)\n",
    "    return ts\n",
    "def getScoreByUserIdAndCaseId(userId,caseId):\n",
    "    \"\"\"\n",
    "    返回某个用户在某道题上的得分,如果用户应该做这道题而没有做,则返回0分,如果这道题这个用户本来就不需要做(即用户所在的group内不包含这道题),就返回-1\n",
    "    :param userId:\n",
    "    :param caseId:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not caseId in groupCaseIds[getGroupIdByUserId(userId)]:\n",
    "        return -1\n",
    "    for i in new_data[userId]:\n",
    "        if i['case_id']==caseId:\n",
    "            return i['final_score']\n",
    "    return 0\n",
    "def getCaseIdsByGroupAndType(groupId,typeId):\n",
    "    return sorted(list(set(caseIdsByType[typeId])&set(groupCaseIds[groupId])))\n",
    "def getUploadNumByUserAndCase(userId,caseId):\n",
    "    if not caseId in groupCaseIds[getGroupIdByUserId(userId)]:\n",
    "        return -1\n",
    "    for i in new_data[userId]:\n",
    "        if i['case_id']==caseId:\n",
    "            return len(i['upload_records'])\n",
    "    return 0\n",
    "def getFinalUploadCodeUrlByUserAndCase(userId,caseId):\n",
    "    if not caseId in groupCaseIds[getGroupIdByUserId(userId)]:\n",
    "        return ''\n",
    "    for i in new_data[userId]:\n",
    "        if i['case_id']==caseId:\n",
    "            return i['upload_records'][-1]['code_url']\n",
    "    return ''\n",
    "def getUploadSumByCaseId(caseId):\n",
    "    r=0\n",
    "    for i in validUserIds:\n",
    "        r+=(getUploadNumByUserAndCase(i,caseId) if getUploadNumByUserAndCase(i,caseId)>0 else 0)\n",
    "    return r\n",
    "def getTimeSpanByUserAndCase(userId,caseId):\n",
    "    if not caseId in groupCaseIds[getGroupIdByUserId(userId)]:\n",
    "        return -2\n",
    "    for i in new_data[userId]:\n",
    "        if i['case_id']==caseId:\n",
    "            return i['upload_records'][-1]['upload_time']-i['upload_records'][0]['upload_time']\n",
    "    return -1\n",
    "def getAvgTimeSpanByCase(caseId):\n",
    "    r=0\n",
    "    n=0\n",
    "    for i in validUserIds:\n",
    "        if getTimeSpanByUserAndCase(i,caseId)>-1:\n",
    "            r+=getTimeSpanByUserAndCase(i,caseId)\n",
    "            n+=1\n",
    "    return r/n\n",
    "typeOneHot=OneHotEncoder(categories='auto').fit([[i] for i in range(8)]).transform([[i] for i in range(8)]).toarray()\n",
    "groupOneHot=OneHotEncoder(categories='auto').fit([[i] for i in range(5)]).transform([[i] for i in range(5)]).toarray()\n",
    "cases_analysis_result=np.array([])\n",
    "def time_diff_minute(firstTime,secondTime):\n",
    "    return (datetime.datetime.strptime(time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime(secondTime/1000)),\"%Y-%m-%d %H:%M:%S\")-datetime.datetime.strptime(time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime(firstTime/1000)),\"%Y-%m-%d %H:%M:%S\")).total_seconds()/60\n",
    "def getTimeSpanByUserAndCaseInMinute(userId,caseId):\n",
    "    if not caseId in groupCaseIds[getGroupIdByUserId(userId)]:\n",
    "        return -2\n",
    "    for i in new_data[userId]:\n",
    "        if i['case_id']==caseId:\n",
    "            return time_diff_minute(i['upload_records'][0]['upload_time'],i['upload_records'][-1]['upload_time'])\n",
    "    return -1\n",
    "def getAvgTimeSpanByCaseInMinute(caseId):\n",
    "    r=0\n",
    "    n=0\n",
    "    for i in validUserIds:\n",
    "        if getTimeSpanByUserAndCaseInMinute(i,caseId)>-1:\n",
    "            r+=getTimeSpanByUserAndCaseInMinute(i,caseId)\n",
    "            n+=1\n",
    "    return r/n\n",
    "for i in range(882):\n",
    "    typeId=np.zeros((8,))\n",
    "    for j in getTypesByCaseId(i):\n",
    "        typeId+=np.array(typeOneHot[j])\n",
    "    finishRate=caseFinishRate[i]\n",
    "    userNum=caseUserNum[i]\n",
    "    userNumInFact=caseUserNumInFact[i]\n",
    "    scoreIgnoreUndo=caseScoreIgnoreUndo[i]\n",
    "    scoreCountUndo=caseScoreCountUndo[i]\n",
    "    groupId=np.zeros((5,))\n",
    "    for j in getGroupIdsByCaseId(i):\n",
    "        groupId+=np.array(groupOneHot[j])\n",
    "    uploadSum=getUploadSumByCaseId(i)\n",
    "    uploadAvg=uploadSum/userNum\n",
    "    uploadAvgInFact=uploadSum/userNumInFact\n",
    "    timeSpan=getAvgTimeSpanByCaseInMinute(i)\n",
    "    caseLine=np.concatenate((np.array([i]),typeId,np.array([finishRate]),np.array([userNum]),np.array([userNumInFact]),np.array([scoreIgnoreUndo]),np.array([scoreCountUndo]),groupId,np.array([uploadSum]),np.array([uploadAvg]),np.array([uploadAvgInFact]),np.array([timeSpan])),axis=0).reshape(1,-1)\n",
    "    if i==0:\n",
    "        cases_analysis_result=caseLine\n",
    "    else:\n",
    "        cases_analysis_result=np.concatenate((cases_analysis_result,caseLine),axis=0)\n",
    "with open('cases_analysis_result.csv',mode='w',newline='') as file:\n",
    "    cw=csv.writer(file)\n",
    "    header=['id','type0','type1','type2','type3','type4','type5','type6','type7','finishRate','userNum','userNumInFact','scoreIgnoreUndo','scoreCountUndo','group0','group1','group2','group3','group4','uploadSum','uploadAvg','uploadAvgInFact','timeSpan']\n",
    "    cw.writerow(header)\n",
    "    for i in cases_analysis_result:\n",
    "        cw.writerow(list(i))\n",
    "\"\"\"\n",
    "cases_analysis_result 数据说明:\n",
    "shape:(882,23)\n",
    "各列含义:\n",
    "id:case_id 0~881\n",
    "type0:如果这道题的类型是0类型,则此列为1,否则为0\n",
    "type1~type7同上\n",
    "finishRate:这道题的完成率 做了这道题的人数/应该做这道题的总人数\n",
    "userNum:应该做这道题的总人数\n",
    "userNumInFact:实际上做了这道题的人数\n",
    "scoreIgnoreUndo:这道题的平均得分,忽略没做的人\n",
    "scoreCountUndo:这道题的平均得分,如果应该做而没有做这道题的人,此题得分记为0\n",
    "group0:如果这道题是第0组中的题目,则为1,否则为0\n",
    "group1~group4同上\n",
    "uploadSum:这道题的提交总次数\n",
    "uploadAvg:平均每个人在这道题上的提交次数,如果有人应该做而没有做这道题,则提交次数记为0\n",
    "uploadAvgInFact:平均每个人在这道题上的提交次数,忽略没有做这道题的人\n",
    "timeSpan:做这道题的平均时间跨度,即最后一次提交时间减去第一次提交时间,单位是分钟\n",
    "\"\"\"\n",
    "cases_analysis_result=pd.read_csv('cases_analysis_result.csv')\n",
    "cases_analysis_result['timeSpan']=StandardScaler().fit_transform(np.array(cases_analysis_result['timeSpan']).reshape(-1,1))\n",
    "def getCodeRunTime(code_url,userId,caseId):\n",
    "    try:\n",
    "        os.chdir('allcases')\n",
    "\n",
    "        dirname=str(userId)+'_'+str(caseId)+'_dir'#存放原压缩包解压物的，每道题都有专属的文件夹名\n",
    "        name=str(userId)+'_'+str(caseId)+'_zip'#原压缩包名\n",
    "\n",
    "        urllib.request.urlretrieve(code_url,name)#下载原压缩包\n",
    "        url_file=zipfile.ZipFile(name)#为原压缩包解压做准备\n",
    "\n",
    "        os.mkdir(dirname)#原压缩包解压目录\n",
    "        os.chdir(dirname)\n",
    "        url_file.extractall()#原压缩包解压\n",
    "\n",
    "        tmp=os.listdir(os.curdir)#当前目录为原压缩包解压目录，即获取原压缩包解压出来的压缩包名\n",
    "        temp=tmp[0]#第二个压缩包名\n",
    "        tempp=zipfile.ZipFile(temp)\n",
    "        tempp.extractall()\n",
    "        #第二个压缩包在此解压\n",
    "\n",
    "        tmp=os.listdir(os.curdir)#再次获取当前目录内的所有文件名，以获得py文件进行运行\n",
    "        code_name=''\n",
    "        for i in tmp:\n",
    "            if i[-3::]=='.py':\n",
    "                code_name=i#py文件名\n",
    "\n",
    "        start_time=time.clock()\n",
    "        win32api.ShellExecute(0,'open',code_name,'','',0)\n",
    "        end_time=time.clock()\n",
    "\n",
    "        os.chdir('..')#返回至allcases目录\n",
    "        os.chdir('..')#返回主目录\n",
    "\n",
    "        return end_time-start_time\n",
    "    except:\n",
    "        os.chdir('..')#返回至allcases目录\n",
    "        os.chdir('..')#返回主目录\n",
    "        return -1\n",
    "# def getCodeRunTime(code_url,userId,caseId):\n",
    "#     return random.uniform(0,1)\n",
    "codeRunTime=-np.ones((271,882))\n",
    "for i in validUserIds:\n",
    "    for j in range(882):\n",
    "        if getFinalUploadCodeUrlByUserAndCase(i,j)!='':\n",
    "            codeRunTime[i,j]=getCodeRunTime(getFinalUploadCodeUrlByUserAndCase(i,j),i,j)\n",
    "            os.chdir(root_path)\n",
    "with open('code_run_time.csv',mode='w',newline='') as file:\n",
    "    cw=csv.writer(file)\n",
    "    for i in codeRunTime:\n",
    "        cw.writerow(list(i))\n",
    "codeRunTime=pd.read_csv('code_run_time.csv',header=None).values\n",
    "def getCaseAvgRunTime(caseId):\n",
    "    sumTime=0\n",
    "    for i in validUserIds:\n",
    "        if codeRunTime[i,caseId]>-1:\n",
    "            sumTime+=codeRunTime[i,caseId]\n",
    "    return sumTime/caseUserNumInFact[caseId]\n",
    "cases_analysis_result['avgRunTime']=np.array([getCaseAvgRunTime(i) for i in range(882)]).reshape(-1,1)\n",
    "def difficult_degree(caseId):\n",
    "    \"\"\"\n",
    "    题目的难度系数,值越大说明题目越难,各列的系数还需要调整\n",
    "    :param caseId:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return 200-cases_analysis_result.iloc[caseId]['finishRate']-cases_analysis_result.iloc[caseId]['scoreIgnoreUndo']-cases_analysis_result.iloc[caseId]['scoreCountUndo']+cases_analysis_result.iloc[caseId]['uploadAvg']+cases_analysis_result.iloc[caseId]['uploadAvgInFact']+cases_analysis_result.iloc[caseId]['timeSpan']+cases_analysis_result.iloc[caseId]['avgRunTime']\n",
    "cases_analysis_result['difficultDegree']=np.array([difficult_degree(i) for i in range(882)]).reshape(-1,1)\n",
    "cases_analysis_result.to_csv('cases_analysis_final.csv')\n",
    "cases_result_array=cases_analysis_result.values"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PCA 降维到4维"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA 4 dim mean square loss:0.6016189131881688\n"
     ]
    }
   ],
   "source": [
    "# new_dim=2\n",
    "new_dim=4\n",
    "# cases_data_pca=PCA(n_components=new_dim).fit_transform(cases_result_array)\n",
    "model_pca=PCA(n_components=new_dim).fit(cases_result_array)\n",
    "cases_data_pca=model_pca.transform(cases_result_array)\n",
    "cases_reconstructed=model_pca.inverse_transform(cases_data_pca)\n",
    "with open('cases_pca_4_dim_result.csv',mode='w',newline='') as file:\n",
    "    cw=csv.writer(file)\n",
    "    header=['id','dim0','dim1','dim2','dim3']\n",
    "    cw.writerow(header)\n",
    "    for i in range(882):\n",
    "        cw.writerow([i,cases_data_pca[i,0],cases_data_pca[i,1],cases_data_pca[i,2],cases_data_pca[i,3]])\n",
    "pca_loss=np.mean(np.square(cases_result_array-cases_reconstructed))\n",
    "print('PCA 4 dim mean square loss:{}'.format(pca_loss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PCA 降维到2维"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA 2 dim mean square loss:31.668025389617327\n"
     ]
    }
   ],
   "source": [
    "new_dim=2\n",
    "# new_dim=4\n",
    "# cases_data_pca=PCA(n_components=new_dim).fit_transform(cases_result_array)\n",
    "model_pca=PCA(n_components=new_dim).fit(cases_result_array)\n",
    "cases_data_pca=model_pca.transform(cases_result_array)\n",
    "cases_reconstructed=model_pca.inverse_transform(cases_data_pca)\n",
    "with open('cases_pca_2_dim_result.csv',mode='w',newline='') as file:\n",
    "    cw=csv.writer(file)\n",
    "    header=['id','dim0','dim1']\n",
    "    cw.writerow(header)\n",
    "    for i in range(882):\n",
    "        cw.writerow([i,cases_data_pca[i,0],cases_data_pca[i,1]])\n",
    "pca_loss=np.mean(np.square(cases_result_array-cases_reconstructed))\n",
    "print('PCA 2 dim mean square loss:{}'.format(pca_loss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 把数据转成Tensor格式"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "cases_tensor=torch.tensor(cases_result_array,dtype=torch.float)\n",
    "cases_dataset=TensorDataset(cases_tensor)\n",
    "cases_sampler=RandomSampler(cases_dataset)\n",
    "cases_dataloader=DataLoader(cases_dataset,sampler=cases_sampler,batch_size=batch_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AE & VAE 降维到2维\n",
    "## AE & VAE 模型定义"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "dim=cases_result_array.shape[1]\n",
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE,self).__init__()\n",
    "        self.encoder=nn.Sequential(\n",
    "            nn.Linear(dim,20),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(20,16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(16,8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(8,4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4,2)\n",
    "        )\n",
    "        self.decoder=nn.Sequential(\n",
    "            nn.Linear(2,4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4,8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(8,16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(16,20),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(20,dim)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        return x\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1=nn.Linear(dim,16)\n",
    "        self.fc21=nn.Linear(16,2)\n",
    "        self.fc22=nn.Linear(16,2)\n",
    "        self.fc3=nn.Linear(2,16)\n",
    "        self.fc4=nn.Linear(16,dim)\n",
    "    def encoder(self,x):\n",
    "        h1=F.relu(self.fc1(x))\n",
    "        return self.fc21(h1),self.fc22(h1)\n",
    "    def reparametrize(self,mu,logvar):\n",
    "        std=logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps=torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps=torch.FloatTensor(std.size()).normal_()\n",
    "        eps=Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    def decode(self,z):\n",
    "        h3=F.relu(self.fc3(z))\n",
    "        return F.sigmoid(self.fc4(h3))\n",
    "    def forward(self,x):\n",
    "        mu,logvar=self.encoder(x)\n",
    "        z=self.reparametrize(mu,logvar)\n",
    "        return self.decode(z),mu,logvar\n",
    "def loss_vae(recon_x,x,mu,logvar,criterion):\n",
    "    \"\"\"\n",
    "    recon_x: generating values\n",
    "    x: origin values\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    mse = criterion(recon_x, x)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    return mse + KLD\n",
    "model_classes={'AE':AE(),'VAE':VAE()}\n",
    "learning_rate=1e-3\n",
    "num_epochs=1\n",
    "# num_epochs=0 # 已经训练好了"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train AE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\60960\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type AE. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1],loss:8572.6416015625\n",
      "Best AE model' loss:8572.6416015625\n"
     ]
    }
   ],
   "source": [
    "model_type='AE'\n",
    "# model_type='VAE'\n",
    "model=model_classes[model_type].cuda()\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "best_loss=np.inf\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for data in cases_dataloader:\n",
    "        batch_cases=data[0].cuda()\n",
    "        output=model(batch_cases)\n",
    "        if model_type=='VAE':\n",
    "            loss=loss_vae(output[0],batch_cases,output[1],output[2],criterion)\n",
    "        else:\n",
    "            loss=criterion(output,batch_cases)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if loss.item()<best_loss:\n",
    "            best_loss=loss.item()\n",
    "            torch.save(model,'best_model_{}_2_dim.pt'.format(model_type))\n",
    "    # if (epoch+1)%50==0:\n",
    "    # print('epoch [{}/{}],loss:{:.4f}'.format(epoch+1,num_epochs,loss.item()))\n",
    "    if (epoch+1)%100==0 or epoch==0:\n",
    "        print('epoch ['+str(epoch+1)+'/'+str(num_epochs)+'],loss:'+str(loss.item()))\n",
    "print('Best '+model_type+' model\\' loss:'+str(best_loss))\n",
    "final_model=torch.load('best_model_AE_2_dim.pt')\n",
    "cases_ae_encoder=final_model.encoder(cases_tensor.cuda()).detach().cpu().numpy()\n",
    "with open('cases_ae_encode_2_dim.csv',mode='w',newline='') as file:\n",
    "    cw=csv.writer(file)\n",
    "    header=['id','dim0','dim1']\n",
    "    cw.writerow(header)\n",
    "    for i in range(882):\n",
    "        cw.writerow([i,cases_ae_encoder[i,0],cases_ae_encoder[i,1]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train VAE\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\60960\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\60960\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type VAE. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1],loss:3325344.0\n",
      "Best VAE model' loss:65218.8125\n"
     ]
    }
   ],
   "source": [
    "# model_type='AE'\n",
    "model_type='VAE'\n",
    "model=model_classes[model_type].cuda()\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "best_loss=np.inf\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for data in cases_dataloader:\n",
    "        batch_cases=data[0].cuda()\n",
    "        output=model(batch_cases)\n",
    "        if model_type=='VAE':\n",
    "            loss=loss_vae(output[0],batch_cases,output[1],output[2],criterion)\n",
    "        else:\n",
    "            loss=criterion(output,batch_cases)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if loss.item()<best_loss:\n",
    "            best_loss=loss.item()\n",
    "            torch.save(model,'best_model_{}_2_dim.pt'.format(model_type))\n",
    "    # if (epoch+1)%50==0:\n",
    "    # print('epoch [{}/{}],loss:{:.4f}'.format(epoch+1,num_epochs,loss.item()))\n",
    "    if (epoch+1)%100==0 or epoch==0:\n",
    "        print('epoch ['+str(epoch+1)+'/'+str(num_epochs)+'],loss:'+str(loss.item()))\n",
    "print('Best '+model_type+' model\\' loss:'+str(best_loss))\n",
    "final_model=torch.load('best_model_VAE_2_dim.pt')\n",
    "cases_vae_encoder=final_model.encoder(cases_tensor.cuda())[0].detach().cpu().numpy()\n",
    "with open('cases_vae_encode_2_dim.csv',mode='w',newline='') as file:\n",
    "    cw=csv.writer(file)\n",
    "    header=['id','dim0','dim1']\n",
    "    cw.writerow(header)\n",
    "    for i in range(882):\n",
    "        cw.writerow([i,cases_vae_encoder[i,0],cases_vae_encoder[i,1]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AE & VAE 降维4维\n",
    "## AE & VAE 模型定义"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "dim=cases_result_array.shape[1]\n",
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE,self).__init__()\n",
    "        self.encoder=nn.Sequential(\n",
    "            nn.Linear(dim,20),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(20,16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(16,8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(8,4)\n",
    "        )\n",
    "        self.decoder=nn.Sequential(\n",
    "            nn.Linear(4,8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(8,16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(16,20),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(20,dim)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        return x\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1=nn.Linear(dim,16)\n",
    "        self.fc21=nn.Linear(16,4)\n",
    "        self.fc22=nn.Linear(16,4)\n",
    "        self.fc3=nn.Linear(4,16)\n",
    "        self.fc4=nn.Linear(16,dim)\n",
    "    def encoder(self,x):\n",
    "        h1=F.relu(self.fc1(x))\n",
    "        return self.fc21(h1),self.fc22(h1)\n",
    "    def reparametrize(self,mu,logvar):\n",
    "        std=logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps=torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps=torch.FloatTensor(std.size()).normal_()\n",
    "        eps=Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    def decode(self,z):\n",
    "        h3=F.relu(self.fc3(z))\n",
    "        return F.sigmoid(self.fc4(h3))\n",
    "    def forward(self,x):\n",
    "        mu,logvar=self.encoder(x)\n",
    "        z=self.reparametrize(mu,logvar)\n",
    "        return self.decode(z),mu,logvar\n",
    "def loss_vae(recon_x,x,mu,logvar,criterion):\n",
    "    \"\"\"\n",
    "    recon_x: generating values\n",
    "    x: origin values\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    mse = criterion(recon_x, x)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    return mse + KLD\n",
    "model_classes={'AE':AE(),'VAE':VAE()}\n",
    "learning_rate=1e-3\n",
    "num_epochs=1\n",
    "# num_epochs=0 # 已经训练好了\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train AE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1],loss:11818.3125\n",
      "Best AE model' loss:7314.03369140625\n"
     ]
    }
   ],
   "source": [
    "model_type='AE'\n",
    "# model_type='VAE'\n",
    "model=model_classes[model_type].cuda()\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "best_loss=np.inf\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for data in cases_dataloader:\n",
    "        batch_cases=data[0].cuda()\n",
    "        output=model(batch_cases)\n",
    "        if model_type=='VAE':\n",
    "            loss=loss_vae(output[0],batch_cases,output[1],output[2],criterion)\n",
    "        else:\n",
    "            loss=criterion(output,batch_cases)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if loss.item()<best_loss:\n",
    "            best_loss=loss.item()\n",
    "            torch.save(model,'best_model_{}_4_dim.pt'.format(model_type))\n",
    "    # if (epoch+1)%50==0:\n",
    "    # print('epoch [{}/{}],loss:{:.4f}'.format(epoch+1,num_epochs,loss.item()))\n",
    "    if (epoch+1)%100==0 or epoch==0:\n",
    "        print('epoch ['+str(epoch+1)+'/'+str(num_epochs)+'],loss:'+str(loss.item()))\n",
    "print('Best '+model_type+' model\\' loss:'+str(best_loss))\n",
    "final_model=torch.load('best_model_AE_4_dim.pt')\n",
    "cases_ae_encoder=final_model.encoder(cases_tensor.cuda()).detach().cpu().numpy()\n",
    "with open('cases_ae_encode_4_dim.csv',mode='w',newline='') as file:\n",
    "    cw=csv.writer(file)\n",
    "    header=['id','dim0','dim1','dim2','dim3']\n",
    "    cw.writerow(header)\n",
    "    for i in range(882):\n",
    "        cw.writerow([i,cases_ae_encoder[i,0],cases_ae_encoder[i,1],cases_ae_encoder[i,2],cases_ae_encoder[i,3]])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train VAE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1],loss:264077888.0\n",
      "Best VAE model' loss:125050736.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model_type='AE'\n",
    "model_type='VAE'\n",
    "model=model_classes[model_type].cuda()\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "best_loss=np.inf\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for data in cases_dataloader:\n",
    "        batch_cases=data[0].cuda()\n",
    "        output=model(batch_cases)\n",
    "        if model_type=='VAE':\n",
    "            loss=loss_vae(output[0],batch_cases,output[1],output[2],criterion)\n",
    "        else:\n",
    "            loss=criterion(output,batch_cases)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if loss.item()<best_loss:\n",
    "            best_loss=loss.item()\n",
    "            torch.save(model,'best_model_{}_2_dim.pt'.format(model_type))\n",
    "    # if (epoch+1)%50==0:\n",
    "    # print('epoch [{}/{}],loss:{:.4f}'.format(epoch+1,num_epochs,loss.item()))\n",
    "    if (epoch+1)%100==0 or epoch==0:\n",
    "        print('epoch ['+str(epoch+1)+'/'+str(num_epochs)+'],loss:'+str(loss.item()))\n",
    "print('Best '+model_type+' model\\' loss:'+str(best_loss))\n",
    "final_model=torch.load('best_model_VAE_2_dim.pt')\n",
    "cases_vae_encoder=final_model.encoder(cases_tensor.cuda())[0].detach().cpu().numpy()\n",
    "with open('cases_vae_encode_2_dim.csv',mode='w',newline='') as file:\n",
    "    cw=csv.writer(file)\n",
    "    header=['id','dim0','dim1','dim2','dim3']\n",
    "    cw.writerow(header)\n",
    "    for i in range(882):\n",
    "        cw.writerow([i,cases_vae_encoder[i,0],cases_vae_encoder[i,1],cases_vae_encoder[i,2],cases_vae_encoder[i,3]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "cases_analysis_final=pd.read_csv('cases_analysis_final.csv')\n",
    "cases_pca_2_dim=pd.read_csv('cases_pca_2_dim_result.csv')\n",
    "cases_pca_4_dim=pd.read_csv('cases_pca_4_dim_result.csv')\n",
    "cases_ae_2_dim=pd.read_csv('cases_ae_encode_2_dim.csv')\n",
    "cases_ae_4_dim=pd.read_csv('cases_ae_encode_4_dim.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\60960\\\\Desktop\\\\数据科学基础大作业\\\\workspace'"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}